{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76757d50-9f4e-4583-86cb-4bf8ee9525e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ LIBRARIES ------------ #\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests as req\n",
    "from urllib.request import urlopen\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a1c81b8-cb26-4267-966c-9e907e41e76d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ FOLDER PATHS ------------ #\n",
    "\n",
    "# Specify your main folder path here\n",
    "main_folder = \"/main/folder\"\n",
    "\n",
    "\n",
    "# Create folder paths from your main folder path\n",
    "assets_folder = main_folder + \"assets/\"\n",
    "\n",
    "censusdata_folder = main_folder + \"censusdata/\"\n",
    "placelevel_data_folder = censusdata_folder + \"Place Level Data/\"\n",
    "yearlevel_data_folder = censusdata_folder + \"Year Level Data/\"\n",
    "contract_rent_data_folder = censusdata_folder + \"Contract Rent Data/\"\n",
    "rent_burden_data_folder = censusdata_folder + \"Rent Burden Data/\"\n",
    "\n",
    "masterfiles_folder = main_folder + \"masterfiles/\"\n",
    "\n",
    "\n",
    "# Create the necessary folders \n",
    "folders = [main_folder, assets_folder, censusdata_folder, yearlevel_data_folder, placelevel_data_folder,\n",
    "           contract_rent_data_folder, rent_burden_data_folder, masterfiles_folder]\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "        \n",
    "# ------------ Notes ------------ #\n",
    "# The assets folder will be where your map and GeoJSON files will be stored. The\n",
    "# censusdata folder will contain all the census data that you deal with, including\n",
    "# the directly extracted datasets, the cleaned datasets, and metadata. From within\n",
    "# this censusdata folder, we create four folders:\n",
    "#\n",
    "# (1) placelevel, which contains census data by places in Los Angeles County\n",
    "# (2) contract_rent, which contains contract rent data\n",
    "# (3) rent_burden, which contains rent burden data\n",
    "# (4) masterfiles\n",
    "#\n",
    "# When we create our websites, we will use data from the masterfiles folder.\n",
    "#\n",
    "# Be sure that the appropriate folder paths are referenced here and when you create\n",
    "# the Dash app(s)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afcb49e1-3e87-4e20-82e1-4278740b612f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ 2020 FIPS CODES FOR CALIFORNIA AND LOS ANGELES COUNTY ------------ #\n",
    "txt_file_url = \"https://www2.census.gov/geo/docs/reference/codes2020/place/st06_ca_place2020.txt\"\n",
    "ca2020 = pd.read_csv(txt_file_url, sep='|', dtype = {'STATEFP': str, 'PLACEFP': str})\n",
    "ca2020['PLACE_FIPS'] = ca2020['STATEFP'] + ca2020['PLACEFP']\n",
    "\n",
    "columns = ['STATE', 'STATEFP', 'PLACEFP', 'PLACE_FIPS']\n",
    "ca2020 = ca2020[columns + [col for col in ca2020.columns if col not in columns]]\n",
    "ca2020['PLACENAME'] = ca2020['PLACENAME'].str.replace(' CDP', \"\")\n",
    "ca2020['PLACENAME'] = ca2020['PLACENAME'].str.replace(' city', \"\")\n",
    "\n",
    "CA_2020_FIPS_filepath = censusdata_folder + \"CA_2020_FIPS.csv\"\n",
    "ca2020.to_csv(CA_2020_FIPS_filepath, index=False)\n",
    "\n",
    "# ALWAYS make sure to have the converters argument, lest the FIPS code be written as an integer!\n",
    "CA_2020_FIPS = pd.read_csv(CA_2020_FIPS_filepath, converters={'STATEFP': str, 'PLACEFP': str, 'PLACE_FIPS': str})\n",
    "\n",
    "# Cities and Census Designated Places (CDPs) in Los Angeles County\n",
    "LosAngelesCounty_2020_FIPS = CA_2020_FIPS[CA_2020_FIPS['COUNTIES'] == ('Los Angeles County')]\n",
    "LosAngelesCounty_2020_FIPS_filepath = censusdata_folder + \"LosAngelesCounty_2020_FIPS.csv\"\n",
    "LosAngelesCounty_2020_FIPS.to_csv(LosAngelesCounty_2020_FIPS_filepath, index=False)\n",
    "\n",
    "# Dictionary of FIPS codes for places in Los Angeles County\n",
    "LosAngelesCounty_dict = LosAngelesCounty_2020_FIPS.set_index('PLACE_FIPS')['PLACENAME'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1583605c-74ee-46bb-a5c9-4e9f5b5f5918",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65b59956-5a8b-491a-a89d-8d35cae446bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ FUNCTION #1 (REQUIRED; extract) ------------ #\n",
    "# Purpose: To download ACS data and metadata\n",
    "\n",
    "def ACS_data_extraction(initial_year, final_year, ACS_ID, FIPS):\n",
    "    \"\"\"\n",
    "    A function that accomplishes three taskes:\n",
    "    (1) Extracting US Census Bureau ACS data for an place (using FIPS) by API key,\n",
    "    (2) Cleaning aforementioned ACS data, and\n",
    "    (3) Extracting as well as formatting the metadata corresponding to each ACS data.\n",
    "\n",
    "    Note that by executing this function, you also create a nested folder within your\n",
    "    directory whose path is as follows:\n",
    "    \n",
    "    path + places/place/ACS_ID/\n",
    "    \n",
    "    The data will be stored in this nested folder, so be sure this updated path is\n",
    "    reflected in your later code!\n",
    "    \"\"\"\n",
    "    years = list(range(initial_year, final_year + 1))\n",
    "    years = list(map(str, years))\n",
    "    urls = []\n",
    "    place = LosAngelesCounty_dict[FIPS].replace(\" \", \"\")\n",
    "\n",
    "    folder_path = f'{placelevel_data_folder}{place}/{ACS_ID}/'\n",
    "    if not os.path.exists(folder_path):\n",
    "        folder = os.makedirs(folder_path)\n",
    "    \n",
    "    # Data extraction\n",
    "    for i in years:\n",
    "        urls.append(f\"https://api.census.gov/data/{i}/acs/acs5?get=group({ACS_ID})&ucgid=pseudo(1600000US{FIPS}$1400000)\")\n",
    "        \n",
    "    \n",
    "    for i, j in zip(urls, years):\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0'\n",
    "        }\n",
    "        \n",
    "        r = req.get(i, headers = headers, stream=True)\n",
    "        file_path = f\"{folder_path}{ACS_ID}_{j}_{place}.csv\"\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for block in r.iter_content(chunk_size=1024):\n",
    "                if block:\n",
    "                    file.write(block)\n",
    "\n",
    "    \n",
    "    # Data cleaning\n",
    "        skip_loop = False\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            if df.empty or df.shape[1] == 0:\n",
    "                skip_loop = True\n",
    "                os.remove(file_path)\n",
    "                continue\n",
    "\n",
    "            df.drop(list(df.filter(regex = 'A$')), axis = 1, inplace = True)\n",
    "            df = df.rename(columns=lambda x: x.strip('][\"'))\n",
    "            for k, name in enumerate(df.columns.to_list()):\n",
    "                if df[name].dtype == 'object':\n",
    "                    df[name] = df[name].str.replace('[\"', \"\")\n",
    "                    df[name] = df[name].str.replace('\"', \"\")\n",
    "                    df[name] = df[name].replace('[null', np.nan)\n",
    "                if name == 'GEO_ID':\n",
    "                    df[name] = df[name].str.replace('1400000US', \"\")\n",
    "                    df[name] = df[name].astype('int64')\n",
    "                if FIPS in name:\n",
    "                    df[name] = df[name].astype('int64')\n",
    "            df['YEAR'] = int(j)\n",
    "            df['FIPS'] = FIPS\n",
    "            value_dict = {-222222222: np.nan, -333333333: np.nan, -666666666: np.nan, '-222222222': np.nan, '-333333333': np.nan, '-666666666': np.nan}\n",
    "            df.replace(value_dict, inplace=True)\n",
    "            ordered_columns = ['YEAR', 'FIPS', 'GEO_ID', 'NAME']\n",
    "            col_list = list(df.filter(regex = f'^{ACS_ID}'))\n",
    "            \n",
    "            df = df[ordered_columns + col_list]\n",
    "\n",
    "            cleaned_file_path = f\"{folder_path}{ACS_ID}_{j}_cleaned_{place}.csv\"\n",
    "            df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            os.remove(file_path)\n",
    "            continue\n",
    "            \n",
    "    \n",
    "        if skip_loop:\n",
    "            os.remove(file_path)\n",
    "            continue\n",
    "    \n",
    "    # Metadata extraction\n",
    "        url = f\"https://api.census.gov/data/{j}/acs/acs5/groups/{ACS_ID}.json\"\n",
    "        \n",
    "        response = urlopen(url)\n",
    "        \n",
    "        data_json = json.loads(urlopen(url).read())\n",
    "        \n",
    "        list1 = [key for key in data_json['variables'].keys()]\n",
    "        list2 = [val.get('label') for val in data_json['variables'].values()]\n",
    "        column_labels = dict(zip(list1, list2))\n",
    "        \n",
    "        df = pd.DataFrame.from_dict(column_labels, orient='index')\n",
    "        df.rename(columns={0: \"Label\"}, inplace=True)\n",
    "        df.index.name = 'ID'\n",
    "        df = df[df[\"Label\"].str.contains(\"Annotation\") == False]\n",
    "        df_new_rows = pd.DataFrame({'Label': ['Year', 'Place-level FIPS code']}, index = ['YEAR', 'FIPS'])\n",
    "        df = pd.concat([df, df_new_rows])\n",
    "        metadata_file = f\"{folder_path}{ACS_ID}_{j}_metadata.csv\"\n",
    "        df.to_csv(metadata_file, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66bad876-75ed-4690-b368-2ce1f6baed7f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ FUNCTION #2 (REQUIRED; mass extract) ------------ #\n",
    "# Purpose: To perform the previous function (downloading ACS data and metadata) en masse\n",
    "def ACS_mass_data_extraction(initial_year, final_year, ACS_ID):\n",
    "    \"\"\"\n",
    "    This function uses our previously defined ACS data extraction function and replicates\n",
    "    it across ALL cities/CDPs in Los Angeles county using the FIPS codes for each place.\n",
    "    It takes four arguments, each of which is the same as that from our aforementioned\n",
    "    function.\n",
    "    \"\"\"\n",
    "    FIPS_list = list(LosAngelesCounty_dict.keys())\n",
    "            \n",
    "    n_dupl = len(FIPS_list)\n",
    "    initial_year_list = [initial_year] * n_dupl\n",
    "    final_year_list = [final_year] * n_dupl\n",
    "    ACS_ID_list = [ACS_ID] * n_dupl\n",
    "    for i, j, k, l in zip(initial_year_list, final_year_list, ACS_ID_list, FIPS_list):\n",
    "        ACS_data_extraction(i, j, k, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "205390ea-5593-40e8-834d-00948cd632d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ FUNCTION #3 (optional) ------------ #\n",
    "# Purpose: To group and collect data by their ACS ID and FIPS code\n",
    "def get_acs_data(initial_year, final_year, ACS_ID, FIPS):\n",
    "    \"\"\"\n",
    "    Group the cleaned ACS data by their ACS ID and produce a dictionary of dataframes.\n",
    "    Each key is a year whose corresponding value is the cleaned dataset. Note that by\n",
    "    assigning a name to the produced dictionary, you can index by the year to get the\n",
    "    dataset for the year of interest.\n",
    "    \"\"\"\n",
    "    years = list(range(initial_year, final_year + 1))\n",
    "    place = LosAngelesCounty_dict[FIPS].replace(\" \", \"\")\n",
    "    acs_data = dict()\n",
    "\n",
    "    \n",
    "    folder_path = f\"{placelevel_data_folder}{place}/{ACS_ID}/\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        folder = os.makedirs(folder_path)\n",
    "    \n",
    "    for year in years:\n",
    "        file = f\"{folder_path}{ACS_ID}_{year}_cleaned_{place}.csv\"\n",
    "        df = pd.read_csv(file, converters = {'FIPS': str})\n",
    "        acs_data[year] = deepcopy(df)\n",
    "\n",
    "    return acs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1247c7bc-8b9b-437f-beed-d89ab5f7bbc6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ FUNCTION #4 (REQUIRED; collect on ACS) ------------ #\n",
    "# Purpose: To concatenate the cleaned FIPS files by their ACS ID\n",
    "def concatenate_ACS_files(ACS_ID, FIPS):\n",
    "    place = LosAngelesCounty_dict[FIPS].replace(\" \", \"\")\n",
    "    extraction_path = f\"{placelevel_data_folder}{place}/{ACS_ID}/\"\n",
    "    string_search = \"cleaned\"\n",
    "    matching_files = [file for file in os.listdir(extraction_path) if \"cleaned\" in file]\n",
    "\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in matching_files:\n",
    "        df = pd.read_csv(extraction_path + file)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index = True)\n",
    "\n",
    "    placename = LosAngelesCounty_dict[FIPS]\n",
    "    combined_df['PLACE'] = placename\n",
    "\n",
    "    ordered_columns = ['YEAR', 'FIPS', 'PLACE', 'GEO_ID', 'NAME']\n",
    "    other_columns = [col for col in combined_df.columns.to_list() if col not in ordered_columns]\n",
    "    combined_df = combined_df[ordered_columns + other_columns]\n",
    "\n",
    "    combined_df = combined_df.sort_values(by = ['YEAR', 'GEO_ID'])\n",
    "\n",
    "    ACS_folder_path = f\"{censusdata_folder}{ACS_ID}/\"\n",
    "    if not os.path.exists(ACS_folder_path):\n",
    "        os.makedirs(ACS_folder_path)\n",
    "        \n",
    "    download_path = ACS_folder_path + f\"{ACS_ID}_{place}.csv\"\n",
    "    combined_df.to_csv(download_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50693aec-ccae-4903-9aa1-e3a8a6ab37bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ FUNCTION #5 (REQUIRED; mass collect) ------------ #\n",
    "# Purpose: To perform the previous function (concatenating the cleaned FIPS files by ACS ID) en masse\n",
    "def mass_concatenate_ACS_files(ACS_ID):\n",
    "    FIPS_list = list(LosAngelesCounty_dict.keys())\n",
    "    n_dupl = len(FIPS_list)\n",
    "    ACS_ID_list = [ACS_ID] * n_dupl\n",
    "    \n",
    "    for i, j in zip(ACS_ID_list, FIPS_list):\n",
    "        concatenate_ACS_files(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2e9cd3b-7e8d-4fa5-b2e7-c32a25b9fe31",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ FUNCTION #6 (REQUIRED; decompose by year and ACS) ------------ #\n",
    "# Purpose: To decompose the previously collected data by year and ACS ID\n",
    "def concatenate_by_year(ACS_ID):\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    folder_path = f\"{censusdata_folder}{ACS_ID}/\"\n",
    "    files = [file for file in sorted(os.listdir(folder_path)) if ACS_ID in file]\n",
    "    for file in files:\n",
    "        file_path = f\"{folder_path}{file}\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index = True)\n",
    "\n",
    "    combined_df['NAME'] = combined_df['NAME'].str.split(',').str[0]\n",
    "    combined_df['NAME'] = combined_df['NAME'].str.split(';').str[0]\n",
    "\n",
    "    years = combined_df['YEAR'].unique().tolist()\n",
    "    for year in years:\n",
    "        mask = combined_df['YEAR'] == year\n",
    "        year_df = combined_df[mask]\n",
    "        download_path = f\"{yearlevel_data_folder}{ACS_ID}_{year}.csv\"\n",
    "        year_df.to_csv(download_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e82e930f-24d5-4f40-b9f8-0bf3aad7a274",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ FUNCTION #7 (REQUIRED; masterfile creation) ------------ #\n",
    "# Purpose: To concatenate all the files in a category into masterfiles\n",
    "def masterfile_concatenate_by_year(list_ACS, category):\n",
    "    \"\"\"\n",
    "    This function takes two arguments\n",
    "    (1) list_ACS, which represents a list of the ACS codes you wish to merge\n",
    "    (2) category, which is either \"contract_rent\" or \"rent_burden\"\n",
    "\n",
    "    If \"contract_rent\" is specified, list the ACS codes: B25057, B25058, B25059.\n",
    "    If \"rent_burden\" is specified, list the ACS codes: B25070, B25072.\n",
    "    \"\"\"\n",
    "    combined_df_list = []\n",
    "    for ACS_ID in list_ACS:\n",
    "        exec(f'{ACS_ID}_combined_df = pd.DataFrame()')\n",
    "        files = [file for file in sorted(os.listdir(yearlevel_data_folder)) if ACS_ID in file]\n",
    "        for file in files:\n",
    "            file_path = f\"{yearlevel_data_folder}{file}\"\n",
    "            df = pd.read_csv(file_path)\n",
    "            exec(f'{ACS_ID}_combined_df = pd.concat([{ACS_ID}_combined_df, df], ignore_index=True)')\n",
    "\n",
    "        exec(f'combined_df_list.append({ACS_ID}_combined_df)')\n",
    "\n",
    "    masterfile = reduce(lambda left, right: pd.merge(left, right, on=['YEAR', 'FIPS', 'PLACE', 'GEO_ID', 'NAME'], how='left'), combined_df_list)\n",
    "\n",
    "    years = masterfile['YEAR'].unique().tolist()\n",
    "    for year in years:\n",
    "        mask = masterfile['YEAR'] == year\n",
    "        year_df = masterfile[mask]\n",
    "        download_path = f\"{masterfiles_folder}{category}_masterfile_{year}.csv\"\n",
    "        year_df.to_csv(download_path, index=False)\n",
    "\n",
    "    return masterfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "908cd40d-8022-4aa3-b4a4-a0616f7e29da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ FUNCTION #8 (optional) ------------ #\n",
    "# Purpose: To obtain map data on each place\n",
    "def get_mapdata(ACS_ID, FIPS):\n",
    "    \"\"\"\n",
    "    We make use of the Census Bureau Geographies website.\n",
    "    Link: https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html\n",
    "    Note that we extract 2024 census tracts for the state of California.\n",
    "\n",
    "    We obtain individual mapdata for each city/CDP listed in Los Angeles County.\n",
    "    \"\"\"\n",
    "    place = LosAngelesCounty_dict[FIPS].replace(\" \", \"\")\n",
    "\n",
    "    LosAngelesCounty_census_tracts = gpd.read_file(assets_folder + \"LosAngelesCounty_census_tracts.json\")\n",
    "\n",
    "    file_path = f\"{censusdata_folder}{ACS_ID}/{ACS_ID}_{place}.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    mask = LosAngelesCounty_census_tracts['GEO_ID'].isin(df['GEO_ID'])\n",
    "    gdf = LosAngelesCounty_census_tracts[mask]\n",
    "\n",
    "    folder_path = f\"{assets_folder}Place Level Mapdata/\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    filename = f\"{folder_path}mapdata_{place}.json\"\n",
    "    gdf.to_file(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c49912d4-461f-470a-a757-d06f5289f307",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ FUNCTION #9 (optional) ------------ #\n",
    "# Purpose: To perform the previous function (getting map data on each place) en masse\n",
    "def mass_get_mapdata(ACS_ID):\n",
    "    \"\"\"\n",
    "    This replicates the previous function, but for all places listed as being\n",
    "    in Los Angeles County during 2024.\n",
    "    \"\"\"\n",
    "    FIPS_list = list(LosAngelesCounty_dict.keys())\n",
    "    n_dupl = len(FIPS_list)\n",
    "    ACS_ID_list = [ACS_ID] * n_dupl\n",
    "\n",
    "    for i, j in zip(ACS_ID_list, FIPS_list):\n",
    "        get_mapdata(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcf226ae-1647-4c20-8cd7-aa5dbe7d6b20",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ FUNCTION #10 (optional; mastergeometry creation) ------------ #\n",
    "# Purpose: To create geometries accompanying each year's masterfile\n",
    "def masterfile_geometries(final_year, initial_year=2010, censusdata_path = censusdata_folder):\n",
    "    \"\"\"\n",
    "    Note that due to redistricting over the years, some census tracts that were\n",
    "    shown to be part of some places in the past may have been relocated. This would\n",
    "    affect our own data and suggest incompleteness when otherwise. As such, we use\n",
    "    the aforementioned Census Bureau Geographies website and extract census tracts\n",
    "    for each year in the state of California and link each JSON file to the respective\n",
    "    masterfile. This ensures that when we develop our app, each place will (1) render\n",
    "    all available census tracts for that year, and (2) render data for said tracts.\n",
    "    \"\"\"\n",
    "    years = range(initial_year, final_year + 1)\n",
    "    geometry_files = []\n",
    "    for year in years:\n",
    "        # California census tracts\n",
    "        filepath = f\"{main_folder}/tl_{year}_06_tract.shp\"\n",
    "        California_census_tracts = gpd.read_file(filepath)\n",
    "        California_census_tracts.columns = California_census_tracts.columns.str.replace('10', '')\n",
    "        # ^ This was done because of 2010\n",
    "        California_census_tracts.drop(['NAME'], axis=1, inplace=True)\n",
    "        California_census_tracts.rename(columns={'GEOID': 'GEO_ID', 'NAMELSAD': 'NAME'}, inplace=True)\n",
    "        California_census_tracts['GEO_ID'] = California_census_tracts['GEO_ID'].astype(int)\n",
    "        California_census_tracts['INTPTLAT'] = California_census_tracts['INTPTLAT'].str.split('+').str[1]\n",
    "        California_census_tracts['INTPTLAT'] = California_census_tracts['INTPTLAT'].astype(float)\n",
    "        California_census_tracts['INTPTLON'] = California_census_tracts['INTPTLON'].astype(float)\n",
    "        California_census_tracts['YEAR'] = year\n",
    "        ordered_columns = ['YEAR']\n",
    "        other_columns = [col for col in California_census_tracts.columns.to_list() if col not in ordered_columns]\n",
    "        California_census_tracts = California_census_tracts[ordered_columns + other_columns]\n",
    "        downloadpath = f\"{main_folder}/CA_{year}_census_tracts.json\"\n",
    "        California_census_tracts.to_file(downloadpath, driver='GeoJSON', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db7abf8c-1061-4395-9185-836b23488651",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ FUNCTION #11 (REQUIRED; mastergeometry specification by year) ------------ #\n",
    "# Purpose: To specify the extracted mastergeometries to correspond to locations based in our masterfiles\n",
    "def mastergeometries_by_year(category):\n",
    "    \"\"\"\n",
    "    Only two possible inputs for the category property: 'contract_rent' or 'rent_burden'\n",
    "    \"\"\"\n",
    "    files = [file for file in sorted(os.listdir(masterfiles_folder)) if category in file]\n",
    "    masterfiles_list = []\n",
    "    for file in files:\n",
    "        masterfile_path = f'{masterfiles_folder}{file}'\n",
    "        masterfiles_list.append(masterfile_path)\n",
    "\n",
    "    geodata_maps = [file for file in sorted(os.listdir(assets_folder)) if 'CA_' in file]\n",
    "    geodata_list = []\n",
    "    for file in geodata_maps:\n",
    "        geometry_path = f'{assets_folder}{file}'\n",
    "        geodata_list.append(geometry_path)\n",
    "\n",
    "    for file, geodata in zip(masterfiles_list, geodata_list):\n",
    "        df = pd.read_csv(file)\n",
    "        gdf = gpd.read_file(geodata)\n",
    "        merged_df = df.merge(gdf, on=['YEAR', 'GEO_ID', 'NAME'], how='left')\n",
    "        merged_df = merged_df[['YEAR', 'FIPS', 'PLACE', 'GEO_ID', 'NAME', 'INTPTLAT', 'INTPTLON', 'geometry']]\n",
    "        year = merged_df.at[0, 'YEAR']\n",
    "        merged_df = gpd.GeoDataFrame(merged_df)\n",
    "        download_path = f'{assets_folder}{category}_mastergeometry_{year}.json'\n",
    "        merged_df.to_file(download_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4f850bc-755f-4bba-9720-9fa8f2b199fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ FUNCTION #11 (REQUIRED; mastergeometry specification by year and place) ------------ #\n",
    "# Purpose: To specify the previously generated year mastergeometries by place.\n",
    "# Reasoning: Clientside callbacks triggered via GitHub Pages will likely consume lots of resources\n",
    "#            by using the year mastergeometries alone. As such, it may be preferential to decompose\n",
    "#            each year mastergeometry by place. The result will be a massive collection of JSON files\n",
    "#            whose file names will correspond to the unique combinations of year and place.\n",
    "\n",
    "def mastergeometries_by_year_place(category):\n",
    "    \"\"\"\n",
    "    Only two possible inputs for the category property: 'contract_rent' or 'rent_burden'\n",
    "    \"\"\"\n",
    "    years = list(range(2010, 2024))\n",
    "    category_path = f'{category}_mastergeometry'\n",
    "    files = [f'{assets_folder}{file}' for file in sorted(os.listdir(assets_folder)) if category_path in file]\n",
    "    for file, year in zip(files, years):\n",
    "        gdf = gpd.read_file(file)\n",
    "        place_list = list(gdf['PLACE'].unique())\n",
    "        folder_path = f'{assets_folder}{category}/{year}/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        \n",
    "        for place in place_list:\n",
    "            dummy_gdf = gdf[gdf['PLACE'] == place]\n",
    "            place = place.replace(\" \", \"\")\n",
    "            download_path = f'{folder_path}{category_path}_{year}_{place}.json'\n",
    "            dummy_gdf.to_file(download_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0790a95-8a85-4541-af65-765a2f5506f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ FUNCTION #12 (optional) ------------ #\n",
    "# Purpose: To convert the previously generated year masterfiles into JSON files specified by year and place.\n",
    "# Reasoning: Same reasoning as function #11.\n",
    "def masterfiles_json(category):\n",
    "    \"\"\"\n",
    "    Only two possible inputs for the category property: 'contract_rent' or 'rent_burden'\n",
    "    \"\"\"\n",
    "    masterfile = pd.DataFrame()\n",
    "    years = range(2010, 2024)\n",
    "    \n",
    "    for year in years:\n",
    "        file_path = f'{masterfiles_folder}{category}_masterfile_{year}.csv'\n",
    "        df = pd.read_csv(file_path)\n",
    "        map_path = f'{assets_folder}{category}_mastergeometry_{year}.json'\n",
    "        gdf = gpd.read_file(map_path)\n",
    "        df = pd.merge(df, gdf[['GEO_ID','INTPTLAT','INTPTLON']], on='GEO_ID', how='left')\n",
    "\n",
    "        if category == 'contract_rent':\n",
    "            df['dummy'] = 1\n",
    "            df['B25058_001E_copy'] = df['B25058_001E']\n",
    "            df['Median'] = df['B25058_001E_copy']\n",
    "            df.drop(columns=['B25058_001E_copy'], inplace=True)\n",
    "            df['75th'] = df['B25059_001E']\n",
    "            df['25th'] = df['B25057_001E']\n",
    "            columns = ['Median', '75th', '25th']\n",
    "            for col in columns:\n",
    "                df[col] = '$' + df[col].astype(str)\n",
    "                df[col] = df[col].str.replace('.0', '')\n",
    "                df.loc[df[col] == '$3501', col] = 'Not available. Exceeds $3500!'\n",
    "                df.loc[df[col] == '$nan', col] = 'Not Available!'\n",
    "                if year in [2010, 2011, 2012, 2013, 2014]:\n",
    "                    df.loc[df[col] == '$2001', col] = 'Not available. Exceeds $2000!'\n",
    "\n",
    "        elif category == 'rent_burden':\n",
    "            df['TotalRentBurden']   = round( ( (df['B25070_007E'] + df['B25070_008E'] + df['B25070_009E'] + df['B25070_010E']) / df['B25070_001E']) * 100, 2)\n",
    "            df['RentBurden_15to24'] = round( ( (df['B25072_006E'] + df['B25072_007E']) / df['B25072_002E']) * 100, 2)\n",
    "            df['RentBurden_25to34'] = round( ( (df['B25072_013E'] + df['B25072_014E']) / df['B25072_009E']) * 100, 2)\n",
    "            df['RentBurden_35to64'] = round( ( (df['B25072_020E'] + df['B25072_021E']) / df['B25072_016E']) * 100, 2)\n",
    "            df['RentBurden_65+']    = round( ( (df['B25072_027E'] + df['B25072_028E']) / df['B25072_023E']) * 100, 2)\n",
    "        \n",
    "            df['RentBurden_15to24_str'] = df['RentBurden_15to24'].astype(str) + '%'\n",
    "            df.loc[df['RentBurden_15to24_str'] == 'nan%', 'RentBurden_15to24_str'] = 'Not Available'\n",
    "            df['RentBurden_25to34_str'] = df['RentBurden_25to34'].astype(str) + '%'\n",
    "            df.loc[df['RentBurden_25to34_str'] == 'nan%', 'RentBurden_25to34_str'] = 'Not Available'\n",
    "            df['RentBurden_35to64_str'] = df['RentBurden_35to64'].astype(str) + '%'\n",
    "            df.loc[df['RentBurden_35to64_str'] == 'nan%', 'RentBurden_35to64_str'] = 'Not Available'\n",
    "            df['RentBurden_65+_str'] = df['RentBurden_65+'].astype(str) + '%'\n",
    "            df.loc[df['RentBurden_65+_str'] == 'nan%', 'RentBurden_65+_str'] = 'Not Available'\n",
    "\n",
    "            df['TotalSevereRentBurden']   = round( ( (df['B25070_010E']) / df['B25070_001E']) * 100, 2)\n",
    "\n",
    "        folder_path = f'{masterfiles_folder}{category}/{year}/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        places = list(df['PLACE'].unique())\n",
    "        for place in places:\n",
    "            dummy_df = df[df['PLACE'] == place]\n",
    "            place = place.replace(\" \", \"\")\n",
    "            download_path = f'{folder_path}{category}_{year}_{place}.json'\n",
    "            dummy_df.to_json(download_path, orient='records', lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ecf3e-d345-409c-ae56-0e041f542e2c",
   "metadata": {},
   "source": [
    "## Applying the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc5552-7110-428d-a772-7a527a6c529d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Contract Rents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7554891e-e20f-4a88-bc61-238e81a72e03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ DATA EXTRACTION ------------ #\n",
    "ACS_mass_data_extraction(2010, 2023, 'B25057')\n",
    "# B25057: Lower Quartile Contract Rent\n",
    "\n",
    "ACS_mass_data_extraction(2010, 2023, 'B25058')\n",
    "# B25058: Median Contract Rent\n",
    "\n",
    "ACS_mass_data_extraction(2010, 2023, 'B25059')\n",
    "# B25059: Upper Quartile Contract Rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceec70b-ed52-49a3-8531-511103afcbbe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------ DATA COLLECTION ------------ #\n",
    "mass_concatenate_ACS_files('B25057')\n",
    "mass_concatenate_ACS_files('B25058')\n",
    "mass_concatenate_ACS_files('B25059')\n",
    "\n",
    "concatenate_by_year('B25057')\n",
    "concatenate_by_year('B25058')\n",
    "concatenate_by_year('B25059')\n",
    "\n",
    "contract_rents_list = ['B25057', 'B25058', 'B25059']\n",
    "masterfile_concatenate_by_year(contract_rents_list, 'contract_rent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0705e4ca-95f7-48f3-9f57-ec8d965c2c14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ ACCOMPANYING MASTER GEOMETRIES ------------ #\n",
    "mastergeometries_by_year('contract_rent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30b035e9-00db-4fe8-8492-54c4a1c5e65b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ MASTER GEOMETRIES BY YEAR & PLACE ------------ #\n",
    "mastergeometries_by_year_place('contract_rent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31e01646-276a-4fe2-b006-a0a555833e00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ MASTERFILES TO JSON FILES BY YEAR & PLACE ------------ #\n",
    "#masterfiles_json('contract_rent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa798515-ff5a-46eb-8c9b-efe950d039ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Rent Burden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c0cc9-a3b8-461f-bf30-952d79a537c4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ DATA EXTRACTION ------------ #\n",
    "ACS_mass_data_extraction(2010, 2023, 'B25070')\n",
    "# B25070: Gross Rent as a Percentage of Household Income in the Past 12 Months\n",
    "\n",
    "ACS_mass_data_extraction(2010, 2023, 'B25072')\n",
    "# B25072: Age of Householder by Gross Rent as a Percentage of Household Income in the Past 12 Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f9a93-e0c6-4c02-b139-f7eb069281d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------ DATA COLLECTION ------------ #\n",
    "mass_concatenate_ACS_files('B25070')\n",
    "mass_concatenate_ACS_files('B25072')\n",
    "\n",
    "concatenate_by_year('B25070')\n",
    "concatenate_by_year('B25072')\n",
    "\n",
    "rent_burden_list = ['B25070', 'B25072']\n",
    "masterfile_concatenate_by_year(rent_burden_list, 'rent_burden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0b1ead6b-ab72-4a52-a75c-0c3a2c03d588",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ ACCOMPANYING MASTER GEOMETRIES ------------ #\n",
    "mastergeometries_by_year('rent_burden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56c29535-8d05-42d0-9408-1524662e3fc6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ MASTER GEOMETRIES BY YEAR & PLACE ------------ #\n",
    "mastergeometries_by_year_place('rent_burden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "251b9bde-b9ef-42d0-9285-92b77cdb6f92",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ------------ MASTERFILES TO JSON FILES BY YEAR & PLACE ------------ #\n",
    "#masterfiles_json('rent_burden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1a781-08aa-4e2b-abaf-8d63ce6a9ea5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
